{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_A1_Q2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-8uWXRco5Yz",
        "outputId": "18d8455a-fa7c-4290-f366-af4db957c459"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ],
      "source": [
        "#Importing all the necessary libraries\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import os\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiFl7ZLEoSJT",
        "outputId": "c38b16a3-f5a5-41a6-a2d3-abcac7b2bf70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting all the files.\n",
        "all_files=os.listdir('/content/drive/MyDrive/IR assignment/Humor,Hist,Media,Food')"
      ],
      "metadata": {
        "id": "AZFbsvhfpJTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dictionary which contains all file names and their respective contents.\n",
        "file_content_dict={'filename':[],'content':[]}\n",
        "for i in all_files:\n",
        "  file1 = open(\"/content/drive/MyDrive/IR assignment/Humor,Hist,Media,Food/\"+i,\"r\",encoding='utf-8',errors='ignore')\n",
        "  file_content_dict['filename'].append(i)\n",
        " # text = file1.read().decode(errors='replace')\n",
        "  text=file1.read()\n",
        "  file_content_dict['content'].append(text)"
      ],
      "metadata": {
        "id": "WFE1MPNRpP9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dataframe which consists two columns i.e., filename and content\n",
        "file_content=pd.DataFrame(file_content_dict)"
      ],
      "metadata": {
        "id": "JzEj2iEWpTgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "MYqMBqvMpVjd",
        "outputId": "0acdca84-afd2-4b6e-c375-c1a2120d3834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8fd133e8-276a-4961-a391-a72ff5cb5572\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rinaldos.txt</td>\n",
              "      <td>\\n                        Rinaldo's Laws\\n    ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bbq.txt</td>\n",
              "      <td>\\n \\n                 AGREEMENT FOR PARTICIPA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mothers.txt</td>\n",
              "      <td>\\n* MOTHER'S DICTIONARY *\\n\\nAMNESIA: conditio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>harmful.hum</td>\n",
              "      <td>DIRTY AIR\\n\\n  Scientists are now saying that ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mowers.txt</td>\n",
              "      <td>\\n\\tDepartment of Agriculture Bulletin #265\\n\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1128</th>\n",
              "      <td>mensroom.jok</td>\n",
              "      <td>From: dwallach@ultra.com (Dan Wallach)\\nNewsgr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1129</th>\n",
              "      <td>female.jok</td>\n",
              "      <td>From nobody@prles2.UUCP Sun Apr  9 15:32:48 19...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1130</th>\n",
              "      <td>goforth.hum</td>\n",
              "      <td>\\cGO FORTH AND WAIT: A play in one scene\\n    ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1131</th>\n",
              "      <td>grommet.hum</td>\n",
              "      <td>HELP! THE GROMMET'S MISSING!\\n\\n  I had a happ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1132</th>\n",
              "      <td>gohome.hum</td>\n",
              "      <td>\\n  In this day and age of overcrowding, ballp...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1133 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8fd133e8-276a-4961-a391-a72ff5cb5572')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8fd133e8-276a-4961-a391-a72ff5cb5572 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8fd133e8-276a-4961-a391-a72ff5cb5572');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "          filename                                            content\n",
              "0     rinaldos.txt  \\n                        Rinaldo's Laws\\n    ...\n",
              "1          bbq.txt   \\n \\n                 AGREEMENT FOR PARTICIPA...\n",
              "2      mothers.txt  \\n* MOTHER'S DICTIONARY *\\n\\nAMNESIA: conditio...\n",
              "3      harmful.hum  DIRTY AIR\\n\\n  Scientists are now saying that ...\n",
              "4       mowers.txt  \\n\\tDepartment of Agriculture Bulletin #265\\n\\...\n",
              "...            ...                                                ...\n",
              "1128  mensroom.jok  From: dwallach@ultra.com (Dan Wallach)\\nNewsgr...\n",
              "1129    female.jok  From nobody@prles2.UUCP Sun Apr  9 15:32:48 19...\n",
              "1130   goforth.hum  \\cGO FORTH AND WAIT: A play in one scene\\n    ...\n",
              "1131   grommet.hum  HELP! THE GROMMET'S MISSING!\\n\\n  I had a happ...\n",
              "1132    gohome.hum  \\n  In this day and age of overcrowding, ballp...\n",
              "\n",
              "[1133 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing**"
      ],
      "metadata": {
        "id": "w1OfYD_M8oa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converted all the words in lower case.\n",
        "# Removed all the special characters and punctuation marks.\n",
        "# Removed all the stopwords from the corpus using NLTK stopwords.\n",
        "# Performed tokenization on the document corpus.\n",
        "\n",
        "w_token = nltk.tokenize.WhitespaceTokenizer()\n",
        "file_content['content']=file_content['content'].str.lower()\n",
        "stop_word = stopwords.words('english')\n",
        "file_content['content'] = file_content['content'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_word)]))\n",
        "file_content['content'] = file_content['content'].str.replace('[^a-zA-Z0-9]',' ')\n",
        "file_content['content_tokens']=file_content['content'].apply(w_token.tokenize)"
      ],
      "metadata": {
        "id": "70-ZXWHppgTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating unique tokens list and token frequency dictionary.\n",
        "token_frequency={}\n",
        "# token_frequency stores the frequency of tokens of the document corpus.\n",
        "unique_tokens=[]\n",
        "# unique_tokens stores the list of unique tokes of the document corpus. \n",
        "for tokens in file_content['content_tokens']:\n",
        "  for word in tokens:\n",
        "          if(word not in token_frequency.keys()):\n",
        "              token_frequency[word] = {}\n",
        "              unique_tokens.append(word)\n",
        "         \n",
        "              "
      ],
      "metadata": {
        "id": "rRMVYnOLqs_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating position index list\n",
        "for i in range(len(all_files)):\n",
        "    term=file_content.iloc[i,2]\n",
        "    for position, word in enumerate(term):\n",
        "        if i in token_frequency[word].keys():\n",
        "            token_frequency[word][i].append(position)\n",
        "        else:\n",
        "            token_frequency[word][i] = [position]"
      ],
      "metadata": {
        "id": "M6YBDjghrE5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Processing the input query**"
      ],
      "metadata": {
        "id": "oQa9T5xGtpoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function takes the query from user and perform all the pre-processing on the query and finally returns the pre-processed query.\n",
        "def EnterQuery():\n",
        "  query=input('Enter your query')\n",
        "  query=query.lower()\n",
        "  query = re.sub('[^a-zA-Z0-9]', ' ', query)\n",
        "  stop_word = stopwords.words('english')\n",
        "  query=query.split()\n",
        "  query_token=[word for word in query if not word in stop_word]\n",
        "  return query_token"
      ],
      "metadata": {
        "id": "FCm1a2OlsNHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_query(arg,query_token):\n",
        "  # The number of tokens = 0: No document retrieved.\n",
        "  if(arg==0):\n",
        "    result=[]\n",
        "    print(\"Number of documents retrieved are: \",len(result))\n",
        "    print(\"The list of document names retrieved are: \",result)  \n",
        "  \n",
        "  # The number of tokens = 1: Simply return the documents containing the token.\n",
        "  elif(arg==1):\n",
        "    if(query_token[0] in unique_tokens):\n",
        "        result = set(token_frequency[query_token[0]].keys())\n",
        "    print(\"Number of documents retrieved are: \",len(result))\n",
        "    document_names=[]\n",
        "    for i in result:\n",
        "      document_names.append(file_content.iloc[i,0])\n",
        "    print(\"The list of document names retrieved are: \",document_names) \n",
        "\n",
        "  # The number of tokens = 2: We check the following condition on all the documents: \n",
        "  #  Position of token2 - Position of token 1 ==  1\n",
        "  elif(arg==2):\n",
        "    result=[]\n",
        "    temp1 = set(token_frequency[query_token[0]].keys())\n",
        "    temp2 = set(token_frequency[query_token[1]].keys())\n",
        "    temp  = list(temp2.intersection(temp1))\n",
        "    for a in temp:\n",
        "      for b in token_frequency[query_token[0]][a]:\n",
        "        for c in token_frequency[query_token[1]][a]:\n",
        "            if(c-b==1):\n",
        "                result.append(a)        \n",
        "    result= list(set(result))\n",
        "    print(\"Number of documents retrieved are: \",len(result))\n",
        "    document_names=[]\n",
        "    for i in result:\n",
        "      document_names.append(file_content.iloc[i,0])\n",
        "    print(\"The list of document names retrieved are: \",document_names)  \n",
        "  \n",
        "#   The number of tokens = 3: We check the following condition on all the documents:\n",
        "#   Position of token3 - Position of token 2 ==  1 and Position of token2 - Position of token 1 ==  1 and Position of token 3 - Position of token 1 ==2.\n",
        "  elif(arg==3):\n",
        "    result=[]\n",
        "    temp1 = set(token_frequency[query_token[0]].keys())\n",
        "    temp2 = set(token_frequency[query_token[1]].keys())\n",
        "    temp3 = set(token_frequency[query_token[2]].keys())\n",
        "    temp4  = temp2.intersection(temp1)\n",
        "    temp = list(temp4.intersection(temp3))\n",
        "    for a in temp:\n",
        "      for b in token_frequency[query_token[0]][a]:\n",
        "        for c in token_frequency[query_token[1]][a]:\n",
        "          for d in token_frequency[query_token[2]][a]:\n",
        "            if(c-b==1 and d-c==1 and d-b==2):\n",
        "                result.append(a)        \n",
        "    result= list(set(result))\n",
        "    print(\"Number of documents retrieved are: \",len(result))\n",
        "    document_names=[]\n",
        "    for i in result:\n",
        "      document_names.append(file_content.iloc[i,0])\n",
        "    print(\"The list of document names retrieved are: \",document_names)  \n",
        "\n",
        "#   The number of tokens = 4: We check the following condition on all the documents:\n",
        "#   Position of token 4 - Position of token 3 ==  1 and Position of token 3 - Position of token 2 ==  1 \n",
        "#   and Position of token 2 - Position of token 1 ==1 and position of token 4 - Position of token 1 == 3.\n",
        "  elif(arg==4):\n",
        "    result=[]\n",
        "    temp1 = set(token_frequency[query_token[0]].keys())\n",
        "    temp2 = set(token_frequency[query_token[1]].keys())\n",
        "    temp3 = set(token_frequency[query_token[2]].keys())\n",
        "    temp5 = set(token_frequency[query_token[3]].keys())\n",
        "    temp4 = temp2.intersection(temp1)\n",
        "    temp6 = temp3.intersection(temp5)\n",
        "    temp = list(temp4.intersection(temp6))\n",
        "    for a in temp:\n",
        "      for b in token_frequency[query_token[0]][a]:\n",
        "        for c in token_frequency[query_token[1]][a]:\n",
        "          for d in token_frequency[query_token[2]][a]:\n",
        "            for e in token_frequency[query_token[3]][a]:\n",
        "              if(c-b==1 and d-c==1 and d-b==2 and e-d==1 and e-c==2 and e-b==3):\n",
        "                  result.append(a)        \n",
        "    result= list(set(result))\n",
        "    print(\"Number of documents retrieved are: \",len(result))\n",
        "    document_names=[]\n",
        "    for i in result:\n",
        "      document_names.append(file_content.iloc[i,0])\n",
        "    print(\"The list of document names retrieved are: \",document_names) \n",
        "\n",
        "#     The number of tokens = 5: We check the following condition on all the documents:\n",
        "#     Position of token 5 - Position of token 4 ==1 and Position of token 4 - Position of token 3 ==  1 \n",
        "#     and Position of token 3 - Position of token 2 ==  1 and Position of token 2 - Position of token 1 ==1 and position of token 5 - Position of token 1 == 4.\n",
        "  \n",
        "  elif(arg==5):\n",
        "    result=[]\n",
        "    temp1 = set(token_frequency[query_token[0]].keys())\n",
        "    temp2 = set(token_frequency[query_token[1]].keys())\n",
        "    temp3 = set(token_frequency[query_token[2]].keys())\n",
        "    temp5 = set(token_frequency[query_token[3]].keys())\n",
        "    temp7 = set(token_frequency[query_token[4]].keys())\n",
        "    temp4 = temp2.intersection(temp1)\n",
        "    temp6 = temp3.intersection(temp5)\n",
        "    temp8=  temp4.intersection(temp6)\n",
        "    temp = list(temp8.intersection(temp7))\n",
        "    for a in temp:\n",
        "      for b in token_frequency[query_token[0]][a]:\n",
        "        for c in token_frequency[query_token[1]][a]:\n",
        "          for d in token_frequency[query_token[2]][a]:\n",
        "            for e in token_frequency[query_token[3]][a]:\n",
        "              for f in token_frequency[query_token[4]][a]:\n",
        "                if(c-b==1 and d-c==1 and d-b==2 and e-d==1 and e-c==2 and e-b==3 and f-e==1 and f-b==4):\n",
        "                    result.append(a)        \n",
        "    result= list(set(result))\n",
        "    print(\"Number of documents retrieved are: \",len(result))\n",
        "    document_names=[]\n",
        "    for i in result:\n",
        "      document_names.append(file_content.iloc[i,0])\n",
        "    print(\"The list of document names retrieved are: \",document_names) \n",
        "  else:\n",
        "    print('We have assumed that the query length is less than or equal to 5.\\nPlease Try again!!!')"
      ],
      "metadata": {
        "id": "tDM5tgmQvd7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=EnterQuery()\n",
        "print(\"The tokens generated from query are: \",query)\n",
        "process_query(len(query),query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4Q7-gdF1Ssy",
        "outputId": "16eff0a1-5719-43de-ef20-818a9c8f6f5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your queryWoods World Championship Chili\n",
            "The tokens generated from query are:  ['woods', 'world', 'championship', 'chili']\n",
            "Number of documents retrieved are:  2\n",
            "The list of document names retrieved are:  ['woods.txt', 'chili.txt']\n"
          ]
        }
      ]
    }
  ]
}