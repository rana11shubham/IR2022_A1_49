{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_A1_Q1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gN4yPMXgi2Il",
        "outputId": "b3d84851-0b94-4b7d-8c16-0ef12142e243"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#Importing all the necessary libraries\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from pathlib import Path\n",
        "import os\n",
        "import glob\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting all the files.\n",
        "all_files=os.listdir('/content/drive/MyDrive/IR assignment/Humor,Hist,Media,Food')\n",
        "\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "w5fPp3M1sHI5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dictionary which contains all file names and their respective contents.\n",
        "file_content_dict={'filename':[],'content':[]}\n",
        "for i in all_files:\n",
        "  file1 = open(\"/content/drive/MyDrive/IR assignment/Humor,Hist,Media,Food/\"+i,\"r\",encoding='utf-8',errors='ignore')\n",
        "  file_content_dict['filename'].append(i)\n",
        " # text = file1.read().decode(errors='replace')\n",
        "  text=file1.read()\n",
        "  file_content_dict['content'].append(text)\n",
        "  \n"
      ],
      "metadata": {
        "id": "PnyBm5d7j60q"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dataframe which consists two columns i.e., filename and content\n",
        "file_content=pd.DataFrame(file_content_dict)"
      ],
      "metadata": {
        "id": "gYwJsSsgrO5D"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_content"
      ],
      "metadata": {
        "id": "b-RoxcYUuVMI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "d21aeebf-7175-46b8-d72c-ee7e2b502edc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ac73b454-30b6-4540-b6f4-78c00a1cc3f1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>apsnet.txt</td>\n",
              "      <td>Date: Wed, 28 Dec 2005 08:16:23 -0700\\nFrom: R...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>applepie.des</td>\n",
              "      <td>Grandma Hauf's Apple Pie\\nCategory: Desserts: ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>anthropo.stu</td>\n",
              "      <td>From duncan@rmy.emory.edu Wed Sep 12 10:22:08 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>apsaucke.des</td>\n",
              "      <td>APPLESAUCE SPICE CAKE\\nCategory: Desserts:  Ca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>antibiot.txt</td>\n",
              "      <td>PROPHYLACTICANTIBIOTICENDOCARDITISMEDICALBACTE...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1128</th>\n",
              "      <td>woods.txt</td>\n",
              "      <td>\\n                 C.V. Woods World Champions...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1129</th>\n",
              "      <td>woodsmok.txt</td>\n",
              "      <td>\"Reno,  Nevada.   Hazardous smog levels yester...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1130</th>\n",
              "      <td>wood</td>\n",
              "      <td>****  The Woody Sketch\\t\\t\\t\\t\\t\\t\\t   ****\\n*...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1131</th>\n",
              "      <td>worldend.hum</td>\n",
              "      <td>The World Ended Yesterday\\n=== ===== ===== ===...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1132</th>\n",
              "      <td>woolly_m.amm</td>\n",
              "      <td>The following article is reproduced from the A...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1133 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac73b454-30b6-4540-b6f4-78c00a1cc3f1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ac73b454-30b6-4540-b6f4-78c00a1cc3f1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ac73b454-30b6-4540-b6f4-78c00a1cc3f1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "          filename                                            content\n",
              "0       apsnet.txt  Date: Wed, 28 Dec 2005 08:16:23 -0700\\nFrom: R...\n",
              "1     applepie.des  Grandma Hauf's Apple Pie\\nCategory: Desserts: ...\n",
              "2     anthropo.stu  From duncan@rmy.emory.edu Wed Sep 12 10:22:08 ...\n",
              "3     apsaucke.des  APPLESAUCE SPICE CAKE\\nCategory: Desserts:  Ca...\n",
              "4     antibiot.txt  PROPHYLACTICANTIBIOTICENDOCARDITISMEDICALBACTE...\n",
              "...            ...                                                ...\n",
              "1128     woods.txt   \\n                 C.V. Woods World Champions...\n",
              "1129  woodsmok.txt  \"Reno,  Nevada.   Hazardous smog levels yester...\n",
              "1130          wood  ****  The Woody Sketch\\t\\t\\t\\t\\t\\t\\t   ****\\n*...\n",
              "1131  worldend.hum  The World Ended Yesterday\\n=== ===== ===== ===...\n",
              "1132  woolly_m.amm  The following article is reproduced from the A...\n",
              "\n",
              "[1133 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing**"
      ],
      "metadata": {
        "id": "w1OfYD_M8oa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying tokenization on text corpus.\n",
        "w_token = nltk.tokenize.WhitespaceTokenizer()"
      ],
      "metadata": {
        "id": "oaBZl6-HCMIm"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization function\n",
        "def tokenized_text(text):\n",
        "    return w_token.tokenize(text)"
      ],
      "metadata": {
        "id": "LrLteUglBmoN"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converted all the words in lower case.\n",
        "file_content['content']=file_content['content'].str.lower()\n",
        "#Removed all the special characters and punctuation marks.\n",
        "file_content['content'] = file_content['content'].str.replace('[^a-zA-Z0-9]',' ')\n",
        "file_content['content'] = file_content['content'].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
        "stop_word = stopwords.words('english')\n",
        "#Removed all the stopwords from the corpus using NLTK stopwords.\n",
        "file_content['content'] = file_content['content'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_word)]))\n",
        "#Performed tokenization on the document corpus.\n",
        "#Performed lemmatization on the tokens of the corpus.\n",
        "file_content['content_tokens']=file_content.content.apply(tokenized_text)"
      ],
      "metadata": {
        "id": "Mw_T-X-29DZN"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "EJgWd7ElCdBm",
        "outputId": "b2d34220-c452-44a4-b46d-01dbbb1d9027"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0aae2a52-355e-45e3-8828-25c25385017c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>content</th>\n",
              "      <th>content_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>apsnet.txt</td>\n",
              "      <td>date wed 28 dec 2005 08 16 23 0700 russ dale j...</td>\n",
              "      <td>[date, wed, 28, dec, 2005, 08, 16, 23, 0700, r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>applepie.des</td>\n",
              "      <td>grandma hauf apple pie category desserts cake ...</td>\n",
              "      <td>[grandma, hauf, apple, pie, category, desserts...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>anthropo.stu</td>\n",
              "      <td>duncan rmy emory edu wed sep 12 10 22 08 1990 ...</td>\n",
              "      <td>[duncan, rmy, emory, edu, wed, sep, 12, 10, 22...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>apsaucke.des</td>\n",
              "      <td>applesauce spice cake category desserts cake c...</td>\n",
              "      <td>[applesauce, spice, cake, category, desserts, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>antibiot.txt</td>\n",
              "      <td>prophylacticantibioticendocarditismedicalbacte...</td>\n",
              "      <td>[prophylacticantibioticendocarditismedicalbact...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1128</th>\n",
              "      <td>woods.txt</td>\n",
              "      <td>c v woods world championship chili 1 3 lb chic...</td>\n",
              "      <td>[c, v, woods, world, championship, chili, 1, 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1129</th>\n",
              "      <td>woodsmok.txt</td>\n",
              "      <td>reno nevada hazardous smog levels yesterday fo...</td>\n",
              "      <td>[reno, nevada, hazardous, smog, levels, yester...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1130</th>\n",
              "      <td>wood</td>\n",
              "      <td>woody sketch monty python flying circus transc...</td>\n",
              "      <td>[woody, sketch, monty, python, flying, circus,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1131</th>\n",
              "      <td>worldend.hum</td>\n",
              "      <td>world ended yesterday l tyler c 1985 author ri...</td>\n",
              "      <td>[world, ended, yesterday, l, tyler, c, 1985, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1132</th>\n",
              "      <td>woolly_m.amm</td>\n",
              "      <td>following article reproduced april 1984 issue ...</td>\n",
              "      <td>[following, article, reproduced, april, 1984, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1133 rows Ã— 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0aae2a52-355e-45e3-8828-25c25385017c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0aae2a52-355e-45e3-8828-25c25385017c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0aae2a52-355e-45e3-8828-25c25385017c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "          filename  ...                                     content_tokens\n",
              "0       apsnet.txt  ...  [date, wed, 28, dec, 2005, 08, 16, 23, 0700, r...\n",
              "1     applepie.des  ...  [grandma, hauf, apple, pie, category, desserts...\n",
              "2     anthropo.stu  ...  [duncan, rmy, emory, edu, wed, sep, 12, 10, 22...\n",
              "3     apsaucke.des  ...  [applesauce, spice, cake, category, desserts, ...\n",
              "4     antibiot.txt  ...  [prophylacticantibioticendocarditismedicalbact...\n",
              "...            ...  ...                                                ...\n",
              "1128     woods.txt  ...  [c, v, woods, world, championship, chili, 1, 3...\n",
              "1129  woodsmok.txt  ...  [reno, nevada, hazardous, smog, levels, yester...\n",
              "1130          wood  ...  [woody, sketch, monty, python, flying, circus,...\n",
              "1131  worldend.hum  ...  [world, ended, yesterday, l, tyler, c, 1985, a...\n",
              "1132  woolly_m.amm  ...  [following, article, reproduced, april, 1984, ...\n",
              "\n",
              "[1133 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating unique tokens list and token frequency dictionary.\n",
        "token_frequency={}\n",
        "# token_frequency stores the frequency of tokens of the document corpus.\n",
        "unique_tokens=[]\n",
        "# unique_tokens stores the list of unique tokes of the document corpus. \n",
        "for tokens in file_content['content_tokens']:\n",
        "  for word in tokens:\n",
        "          if(word in token_frequency.keys()):\n",
        "              token_frequency[word] = token_frequency[word] + 1\n",
        "          else:\n",
        "              token_frequency[word] = 1\n",
        "              unique_tokens.append(word)"
      ],
      "metadata": {
        "id": "R3zsWG1R_OU1"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating posting list for each term\n",
        "posting_list={}\n",
        "# posting_list contains the list of documents to which particular token belongs to.\n",
        "for word in unique_tokens:\n",
        "    posting_list[word] = []\n",
        "\n",
        "for i in range(len(file_content)):\n",
        "    tokens = file_content.iloc[i,2]\n",
        "    for word in tokens:\n",
        "        if(i not in posting_list[word]):\n",
        "            posting_list[word].append(i) \n"
      ],
      "metadata": {
        "id": "0j2NxQ1sEGAU"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Handling the queries**"
      ],
      "metadata": {
        "id": "W8r1LZmqeHCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The AND operator function takes 2 posting lists as an argument.\n",
        "# These posting lists are already sorted in ascending order of doc ids.\n",
        "# We have used 2 pointer approach to intersect both the posting lists. If we found the equal doc ids, we append it in the result, otherwise, we increment the pointer of smaller doc id posting list. \n",
        "# While we are comparing, we have also maintained one variable for the comparison where we increment the variable as we go on comparing the posting list.\n",
        "\n",
        "def AND_operation(posting_List1, posting_List2): \n",
        "  comparison = 0\n",
        "  results=[]\n",
        "  i=0\n",
        "  j=0\n",
        "  while(i<len(posting_List1) and j<len(posting_List2)): \n",
        "    if(posting_List1[i]<posting_List2[j]): \n",
        "        comparison+=1\n",
        "        i+=1\n",
        "    else:\n",
        "      if(posting_List1[i]==posting_List2[j]): \n",
        "        comparison+=1\n",
        "        results.append(posting_List1[i]) \n",
        "        i+=1\n",
        "        j+=1\n",
        "      else:\n",
        "        comparison+=1\n",
        "        j+=1\n",
        "  return results, comparison"
      ],
      "metadata": {
        "id": "n2op8yGXeGGc"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The OR operator function takes 2 posting lists as an argument.\n",
        "# These posting lists are already sorted in ascending order of doc ids.\n",
        "# We have used 2 pointer approach to find the union of both the posting lists. If we found the equal doc ids, we append it in the result, otherwise, we append and increment the pointer of smaller doc id posting list. \n",
        "# While we are comparing, we have also maintained one variable for the comparison where we increment the variable as we go on comparing the posting list.\n",
        "\n",
        "def OR_operation(posting_List1, posting_List2): \n",
        "  comparison = 0\n",
        "  results=[]\n",
        "  i=0\n",
        "  j=0\n",
        "  while(i<len(posting_List1) and j<len(posting_List2)): \n",
        "    # posting_List1[i] = int(posting_List1[i])\n",
        "    # posting_List2[j] = int(posting_List2[j])\n",
        "    if(posting_List1[i]<posting_List2[j]): \n",
        "        comparison+=1\n",
        "        results.append(posting_List1[i]) \n",
        "        i+=1\n",
        "    else:\n",
        "      if(posting_List1[i]==posting_List2[j]): \n",
        "        comparison+=1\n",
        "        results.append(posting_List1[i]) \n",
        "        i+=1\n",
        "        j+=1\n",
        "      else:\n",
        "        comparison+=1\n",
        "        results.append(posting_List2[j]) \n",
        "        j+=1\n",
        "  return results, comparison"
      ],
      "metadata": {
        "id": "h6oYmonqftx4"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The NOT operator function takes 1 posting list as an argument.\n",
        "# The posting list is already sorted in ascending order of doc ids.\n",
        "# We have found the difference between the list that contains all doc ids and this posting list and then return the result.\n",
        "\n",
        "def NOT_operation(posting_List):\n",
        "    indexes = []\n",
        "    results =[]\n",
        "    for i in range(len(all_files)): # Creates a temporary result list containing all the indexes\n",
        "        indexes.append(i)\n",
        "    main_index =0\n",
        "    posting_index =0\n",
        "    for i in range(posting_List[len(posting_List)-1]):\n",
        "        # print(cur_index_main)\n",
        "        if(indexes[main_index]==posting_List[posting_index] ):\n",
        "            posting_index += 1\n",
        "            main_index += 1\n",
        "        else:\n",
        "            results.append(indexes[main_index])\n",
        "            main_index +=1\n",
        "    results = results+indexes[main_index+1:]\n",
        "    return results"
      ],
      "metadata": {
        "id": "A6b0zUmYgDj2"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The AND NOT operator function takes 2 posting lists A and B as an argument.\n",
        "# These posting lists are already sorted in ascending order of doc ids.\n",
        "# First, we calculate the NOT of posting list B from the NOT operation function and store the immediate result.\n",
        "# After that, we perform the AND operation between posting list A and the immediate NOT result of posting list B.\n",
        "\n",
        "def AND_NOT_operation(posting_List1,posting_List2):\n",
        "\n",
        "  posting_List3 = NOT_operation(posting_List2) \n",
        "  results,comparison = AND_operation(posting_List1, posting_List3) \n",
        "  return results,comparison\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WT-1k16QiiXQ"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The OR NOT operator function takes 2 posting lists A and B as an argument.\n",
        "# These posting lists are already sorted in ascending order of doc ids.\n",
        "# First, we calculate the NOT of posting list B from the NOT operation function and store the immediate result.\n",
        "# After that, we perform the OR operation between posting list A and the immediate NOT result of posting list B.\n",
        "\n",
        "def OR_NOT_operation(posting_List1, posting_List2):\n",
        "\n",
        "    posting_List3 = NOT_operation(posting_List2)\n",
        "    results,comparison = OR_operation(posting_List1, posting_List3)\n",
        "    return results,comparison"
      ],
      "metadata": {
        "id": "8-f-L0x8i9IB"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function takes the query from user and perform all the pre-processing on the query and finally returns the pre-processed query.\n",
        "def EnterQuery():\n",
        "  query=input('Enter your query')\n",
        "  query=query.lower()\n",
        "  query = re.sub('[^a-zA-Z0-9]', ' ', query)\n",
        "  # query = query.replace('http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
        "  stop_word = stopwords.words('english')\n",
        "  # query = query.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_word)]))\n",
        "  query=query.split()\n",
        "  query_token = [word for word in query if not word in stop_word]\n",
        "  # query_token=query.apply(lemmatize_text)\n",
        "  return query_token"
      ],
      "metadata": {
        "id": "vr7JHPEAkUNg"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function takes the list of operators from the user which should be one less than the query tokens and applied necessary processing\n",
        "# to convert it into tokens and finally return it.\n",
        "def EnterOperations(tokens):\n",
        "    operations = input(f'Enter the operators list of {tokens-1} operators')\n",
        "    operations = operations.replace(']','') \n",
        "    operations = operations.replace('[','')\n",
        "    operations = operations.replace(' ','')\n",
        "    operations = operations.lower()\n",
        "    operations = operations.split(',')\n",
        "    return operations\n",
        "    "
      ],
      "metadata": {
        "id": "So0N973jkKGx"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function takes three arguments like two posting lists and one operator and called necessary functions according to the\n",
        "# value of operator and finally returns the result and number of comparisons.\n",
        "def Processing_input(posting_List1, posting_List2,operator):\n",
        "    if(operator == 'or'):\n",
        "        result, comparison = OR_operation(posting_List1, posting_List2) \n",
        "    if(operator == 'and'): \n",
        "        result, comparison = AND_operation(posting_List1, posting_List2) \n",
        "    if(operator == 'ornot'):\n",
        "        result, comparison = OR_NOT_operation(posting_List1, posting_List2)\n",
        "    if(operator == 'andnot'):\n",
        "        result, comparison= AND_NOT_operation(posting_List1, posting_List2) \n",
        "    return result, comparison"
      ],
      "metadata": {
        "id": "wrkMdESwnsV5"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function takes the number of queries and perform required operations.\n",
        "try:\n",
        "  comparisons=0\n",
        "  N=int(input('Enter the number of queries'))\n",
        "  for i in range(N):\n",
        "    query=EnterQuery()\n",
        "\n",
        "    operations=EnterOperations(len(query))\n",
        "    for i in range(len(operations)):\n",
        "        if(i == 0):\n",
        "            result = posting_list[query[0]]  \n",
        "            # num_comp=num_comp+num_comp_temp  \n",
        "        result,comp = Processing_input(result, posting_list[query[i+1]],operations[i])\n",
        "        comparisons=comparisons+comp\n",
        "    print(\"Number of documents retrieved:\",len(result))\n",
        "    print(\"Minimum number of comparisons done:\",comparisons)\n",
        "    document_names=[]\n",
        "    for i in result:\n",
        "      document_names.append(file_content.iloc[i,0])\n",
        "    print(\"The list of document names retrieved: \",document_names)\n",
        "except:\n",
        "  print('Enter Valid Input!!!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P84s2Jadkqxy",
        "outputId": "4f8d17e4-88b3-4984-f648-a45d237d341d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of queries1\n",
            "Enter your querylion stood thoughtfully for a moment\n",
            "Enter the operators list of 3 operators[ OR, OR , OR ]\n",
            "Number of documents retrieved: 167\n",
            "Minimum number of comparisons done: 347\n",
            "The list of document names retrieved:  ['anim_lif.txt', 'annoy.fascist', 'anime.lif', 'ambrose.bie', 'bbh_intv.txt', 'barney.txt', 'a_tv_t-p.com', 'b-2.jok', 'art-fart.hum', 'bmdn01.txt', 'bitnet.txt', 'beesherb.txt', 'beauty.tm', 'cabbage.txt', 'caesardr.sal', 'bw.txt', 'bw-phwan.hat', 'butwrong.hum', 'boneles2.txt', 'booze1.fun', 'christop.int', 'childhoo.jok', 'chickenheadbbs.txt', 'candy.txt', 'calculus.txt', 'consp.txt', 'conan.txt', 'computer.txt', 'collected_quotes.txt', 'cogdis.txt', 'cmu.share', 'clancy.txt', 'cybrtrsh.txt', 'coyote.txt', 'cuchy.hum', 'cookie.1', 'dthought.txt', 'dromes.txt', 'drinks.gui', 'doggun.sto', 'deep.txt', 'devils.jok', 'dead5.txt', 'dead4.txt', 'facedeth.txt', 'exam.50', 'epi_tton.txt', 'eskimo.nel', 'epi_.txt', 'epitaph', 'english.txt', 'engineer.hum', 'econridl.fun', 'flux_fix.txt', 'filmgoof.txt', 'fascist.txt', 'golnar.txt', 'gd_ql.txt', 'ghostfun.hum', 'gd_gal.txt', 'gas.txt', 'fuckyou2.txt', 'homebrew.txt', 'hackingcracking.txt', 'hackmorality.txt', 'grail.txt', 'gown.txt', 'insults1.txt', 'incarhel.hum', 'insult.lst', 'indgrdn.txt', 'idr2.txt', 'humor9.txt', 'kanalx.txt', 'kaboom.hum', 'jayjay.txt', 'jc-elvis.inf', 'japantv.txt', 'is_story.txt', 'iremember', 'ivan.hum', 'lion.txt', 'lions.cat', 'lifeonledge.txt', 'lion.jok', 'letgosh.txt', 'lifeimag.hum', 'lif&love.hum', 'let.go', 'lawyer.jok', 'lbinter.hum', 'marriage.hum', 'mash.hum', 'maecenas.hum', 'mailfrag.hum', 'm0dzmen.hum', 'luvstory.txt', 'luggage.hum', 'lozerzon.hum', 'llong.hum', 'moose.txt', 'moore.txt', 'montpyth.hum', 'mlverb.hum', 'minn.txt', 'misc.1', 'mindvox', 'mel.txt', 'meinkamp.hum', 'nigel10.txt', 'nihgel_8.9', 'nigel.5', 'nigel.3', 'nigel.2', 'nigel.10', 'news.hum', 'namaste.txt', 'nameisreo.txt', 'myheart.hum', 'murphys.txt', 'murphy_l.txt', 'mundane.v2', 'msorrow', 'onetotwo.hum', 'onetoone.hum', 'oldeng.hum', 'pizzawho.hum', 'policpig.hum', 'pepper.txt', 'pepsideg.txt', 'phorse.hum', 'petshop', 'peatchp.hum', 'passage.hum', 'passenge.sim', 'oxymoron.jok', 'quack26.txt', 'psycho.txt', 'psych_pr.quo', 'progrs.gph', 'pro-fact.hum', 'practica.txt', 'pracjoke.txt', 'prac3.jok', 'prac2.jok', 'prac1.jok', 'popmusi.hum', 'reasons.txt', 'quotes.txt', 'quux_p.oem', 'quest.hum', 'puzzles.jok', 'rns_ency.txt', 'scratchy.txt', 'reeves.txt', 'soleleer.hum', 'socecon.hum', 'solders.hum', 'smurfkil.hum', 'snapple.rum', 'sfmovie.txt', \"terrmcd'.hum\", 'sw_err.txt', 'suicide2.txt', 'stuf10.txt', 'stuf11.txt', 'strine.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GlIPrhsPvLD-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}