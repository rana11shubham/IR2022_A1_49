{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_A1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gN4yPMXgi2Il",
        "outputId": "0c0bf84b-58f6-4704-86d8-1854807fcdd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ],
      "source": [
        "#Importing all the necessary libraries\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from pathlib import Path\n",
        "import os\n",
        "import glob\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting all the files.\n",
        "all_files=os.listdir('/content/drive/MyDrive/IR assignment/Humor,Hist,Media,Food')\n",
        "\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "w5fPp3M1sHI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dictionary which contains all file names and their respective contents.\n",
        "file_content_dict={'filename':[],'content':[]}\n",
        "for i in all_files:\n",
        "  file1 = open(\"/content/drive/MyDrive/IR assignment/Humor,Hist,Media,Food/\"+i,\"r\",encoding='utf-8',errors='ignore')\n",
        "  file_content_dict['filename'].append(i)\n",
        " # text = file1.read().decode(errors='replace')\n",
        "  text=file1.read()\n",
        "  file_content_dict['content'].append(text)\n",
        "  \n"
      ],
      "metadata": {
        "id": "PnyBm5d7j60q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dataframe which consists two columns i.e., filename and content\n",
        "file_content=pd.DataFrame(file_content_dict)"
      ],
      "metadata": {
        "id": "gYwJsSsgrO5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_content"
      ],
      "metadata": {
        "id": "b-RoxcYUuVMI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "606cfe90-fbda-4d3f-bdfc-cd464466a237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ebe3c1de-6576-4e20-901b-a9426841042a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rinaldos.txt</td>\n",
              "      <td>\\n                        Rinaldo's Laws\\n    ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bbq.txt</td>\n",
              "      <td>\\n \\n                 AGREEMENT FOR PARTICIPA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mothers.txt</td>\n",
              "      <td>\\n* MOTHER'S DICTIONARY *\\n\\nAMNESIA: conditio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>harmful.hum</td>\n",
              "      <td>DIRTY AIR\\n\\n  Scientists are now saying that ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mowers.txt</td>\n",
              "      <td>\\n\\tDepartment of Agriculture Bulletin #265\\n\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1128</th>\n",
              "      <td>mensroom.jok</td>\n",
              "      <td>From: dwallach@ultra.com (Dan Wallach)\\nNewsgr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1129</th>\n",
              "      <td>female.jok</td>\n",
              "      <td>From nobody@prles2.UUCP Sun Apr  9 15:32:48 19...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1130</th>\n",
              "      <td>goforth.hum</td>\n",
              "      <td>\\cGO FORTH AND WAIT: A play in one scene\\n    ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1131</th>\n",
              "      <td>grommet.hum</td>\n",
              "      <td>HELP! THE GROMMET'S MISSING!\\n\\n  I had a happ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1132</th>\n",
              "      <td>gohome.hum</td>\n",
              "      <td>\\n  In this day and age of overcrowding, ballp...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1133 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ebe3c1de-6576-4e20-901b-a9426841042a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ebe3c1de-6576-4e20-901b-a9426841042a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ebe3c1de-6576-4e20-901b-a9426841042a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "          filename                                            content\n",
              "0     rinaldos.txt  \\n                        Rinaldo's Laws\\n    ...\n",
              "1          bbq.txt   \\n \\n                 AGREEMENT FOR PARTICIPA...\n",
              "2      mothers.txt  \\n* MOTHER'S DICTIONARY *\\n\\nAMNESIA: conditio...\n",
              "3      harmful.hum  DIRTY AIR\\n\\n  Scientists are now saying that ...\n",
              "4       mowers.txt  \\n\\tDepartment of Agriculture Bulletin #265\\n\\...\n",
              "...            ...                                                ...\n",
              "1128  mensroom.jok  From: dwallach@ultra.com (Dan Wallach)\\nNewsgr...\n",
              "1129    female.jok  From nobody@prles2.UUCP Sun Apr  9 15:32:48 19...\n",
              "1130   goforth.hum  \\cGO FORTH AND WAIT: A play in one scene\\n    ...\n",
              "1131   grommet.hum  HELP! THE GROMMET'S MISSING!\\n\\n  I had a happ...\n",
              "1132    gohome.hum  \\n  In this day and age of overcrowding, ballp...\n",
              "\n",
              "[1133 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing**"
      ],
      "metadata": {
        "id": "w1OfYD_M8oa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying lemmatization on text corpus.\n",
        "wnl = WordNetLemmatizer()\n",
        "w_token = nltk.tokenize.WhitespaceTokenizer()"
      ],
      "metadata": {
        "id": "oaBZl6-HCMIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization and Tokenization function\n",
        "def lemmatize_text(text):\n",
        "    return [wnl.lemmatize(w) for w in w_token.tokenize(text)]"
      ],
      "metadata": {
        "id": "LrLteUglBmoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converted all the words in lower case.\n",
        "file_content['content']=file_content['content'].str.lower()\n",
        "#Removed all the special characters and punctuation marks.\n",
        "file_content['content'] = file_content['content'].str.replace('[^a-zA-Z0-9]',' ')\n",
        "file_content['content'] = file_content['content'].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
        "stop_word = stopwords.words('english')\n",
        "#Removed all the stopwords from the corpus using NLTK stopwords.\n",
        "file_content['content'] = file_content['content'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_word)]))\n",
        "#Performed tokenization on the document corpus.\n",
        "#Performed lemmatization on the tokens of the corpus.\n",
        "file_content['content_tokens']=file_content.content.apply(lemmatize_text)"
      ],
      "metadata": {
        "id": "Mw_T-X-29DZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating unique tokens list and token frequency dictionary.\n",
        "token_frequency={}\n",
        "# token_frequency stores the frequency of tokens of the document corpus.\n",
        "unique_tokens=[]\n",
        "# unique_tokens stores the list of unique tokes of the document corpus. \n",
        "for tokens in file_content['content_tokens']:\n",
        "  for word in tokens:\n",
        "          if(word in token_frequency.keys()):\n",
        "              token_frequency[word] = token_frequency[word] + 1\n",
        "          else:\n",
        "              token_frequency[word] = 1\n",
        "              unique_tokens.append(word)"
      ],
      "metadata": {
        "id": "R3zsWG1R_OU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating posting list for each term\n",
        "posting_list={}\n",
        "# posting_list contains the list of documents to which particular token belongs to.\n",
        "for word in unique_tokens:\n",
        "    posting_list[word] = []\n",
        "\n",
        "for i in range(len(file_content)):\n",
        "    tokens = file_content.iloc[i,2]\n",
        "    for word in tokens:\n",
        "        if(i not in posting_list[word]):\n",
        "            posting_list[word].append(i) \n"
      ],
      "metadata": {
        "id": "0j2NxQ1sEGAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Handling the queries**"
      ],
      "metadata": {
        "id": "W8r1LZmqeHCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The AND operator function takes 2 posting lists as an argument.\n",
        "# These posting lists are already sorted in ascending order of doc ids.\n",
        "# We have used 2 pointer approach to intersect both the posting lists. If we found the equal doc ids, we append it in the result, otherwise, we increment the pointer of smaller doc id posting list. \n",
        "# While we are comparing, we have also maintained one variable for the comparison where we increment the variable as we go on comparing the posting list.\n",
        "\n",
        "def AND_operation(posting_List1, posting_List2): \n",
        "  comparison = 0\n",
        "  results=[]\n",
        "  i=0\n",
        "  j=0\n",
        "  while(i<len(posting_List1) and j<len(posting_List2)): \n",
        "    if(posting_List1[i]<posting_List2[j]): \n",
        "        comparison+=1\n",
        "        i+=1\n",
        "    else:\n",
        "      if(posting_List1[i]==posting_List2[j]): \n",
        "        comparison+=1\n",
        "        results.append(posting_List1[i]) \n",
        "        i+=1\n",
        "        j+=1\n",
        "      else:\n",
        "        comparison+=1\n",
        "        j+=1\n",
        "  return results, comparison"
      ],
      "metadata": {
        "id": "n2op8yGXeGGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The OR operator function takes 2 posting lists as an argument.\n",
        "# These posting lists are already sorted in ascending order of doc ids.\n",
        "# We have used 2 pointer approach to find the union of both the posting lists. If we found the equal doc ids, we append it in the result, otherwise, we append and increment the pointer of smaller doc id posting list. \n",
        "# While we are comparing, we have also maintained one variable for the comparison where we increment the variable as we go on comparing the posting list.\n",
        "\n",
        "def OR_operation(posting_List1, posting_List2): \n",
        "  comparison = 0\n",
        "  results=[]\n",
        "  i=0\n",
        "  j=0\n",
        "  while(i<len(posting_List1) and j<len(posting_List2)): \n",
        "    # posting_List1[i] = int(posting_List1[i])\n",
        "    # posting_List2[j] = int(posting_List2[j])\n",
        "    if(posting_List1[i]<posting_List2[j]): \n",
        "        comparison+=1\n",
        "        results.append(posting_List1[i]) \n",
        "        i+=1\n",
        "    else:\n",
        "      if(posting_List1[i]==posting_List2[j]): \n",
        "        comparison+=1\n",
        "        results.append(posting_List1[i]) \n",
        "        i+=1\n",
        "        j+=1\n",
        "      else:\n",
        "        comparison+=1\n",
        "        results.append(posting_List2[j]) \n",
        "        j+=1\n",
        "  return results, comparison"
      ],
      "metadata": {
        "id": "h6oYmonqftx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The NOT operator function takes 1 posting list as an argument.\n",
        "# The posting list is already sorted in ascending order of doc ids.\n",
        "# We have found the difference between the list that contains all doc ids and this posting list and then return the result.\n",
        "\n",
        "def NOT_operation(posting_List):\n",
        "    indexes = []\n",
        "    results =[]\n",
        "    for i in range(len(all_files)): # Creates a temporary result list containing all the indexes\n",
        "        indexes.append(i)\n",
        "    main_index =0\n",
        "    posting_index =0\n",
        "    for i in range(posting_List[len(posting_List)-1]):\n",
        "        # print(cur_index_main)\n",
        "        if(indexes[main_index]==posting_List[posting_index] ):\n",
        "            posting_index += 1\n",
        "            main_index += 1\n",
        "        else:\n",
        "            results.append(indexes[main_index])\n",
        "            main_index +=1\n",
        "    results = results+indexes[main_index+1:]\n",
        "    return results"
      ],
      "metadata": {
        "id": "A6b0zUmYgDj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The AND NOT operator function takes 2 posting lists A and B as an argument.\n",
        "# These posting lists are already sorted in ascending order of doc ids.\n",
        "# First, we calculate the NOT of posting list B from the NOT operation function and store the immediate result.\n",
        "# After that, we perform the AND operation between posting list A and the immediate NOT result of posting list B.\n",
        "\n",
        "def AND_NOT_operation(posting_List1,posting_List2):\n",
        "\n",
        "  posting_List3 = NOT_operation(posting_List2) \n",
        "  results,comparison = AND_operation(posting_List1, posting_List3) \n",
        "  return results,comparison\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WT-1k16QiiXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The OR NOT operator function takes 2 posting lists A and B as an argument.\n",
        "# These posting lists are already sorted in ascending order of doc ids.\n",
        "# First, we calculate the NOT of posting list B from the NOT operation function and store the immediate result.\n",
        "# After that, we perform the OR operation between posting list A and the immediate NOT result of posting list B.\n",
        "\n",
        "def OR_NOT_operation(posting_List1, posting_List2):\n",
        "\n",
        "    posting_List3 = NOT_operation(posting_List2)\n",
        "    results,comparison = OR_operation(posting_List1, posting_List3)\n",
        "    return results,comparison"
      ],
      "metadata": {
        "id": "8-f-L0x8i9IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function takes the query from user and perform all the pre-processing on the query and finally returns the pre-processed query.\n",
        "def EnterQuery():\n",
        "  query=input('Enter your query')\n",
        "  query=query.lower()\n",
        "  query = re.sub('[^a-zA-Z0-9]', ' ', query)\n",
        "  # query = query.replace('http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
        "  stop_word = stopwords.words('english')\n",
        "  # query = query.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_word)]))\n",
        "  query=query.split()\n",
        "  query_token = [wnl.lemmatize(word) for word in query if not word in stop_word]\n",
        "  # query_token=query.apply(lemmatize_text)\n",
        "  return query_token"
      ],
      "metadata": {
        "id": "vr7JHPEAkUNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function takes the list of operators from the user which should be one less than the query tokens and applied necessary processing\n",
        "# to convert it into tokens and finally return it.\n",
        "def EnterOperations(tokens):\n",
        "    operations = input(f'Enter the operators list of {tokens-1} operators')\n",
        "    operations = operations.replace(']','') \n",
        "    operations = operations.replace('[','')\n",
        "    operations = operations.replace(' ','')\n",
        "    operations = operations.lower()\n",
        "    operations = operations.split(',')\n",
        "    return operations\n",
        "    "
      ],
      "metadata": {
        "id": "So0N973jkKGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function takes three arguments like two posting lists and one operator and called necessary functions according to the\n",
        "# value of operator and finally returns the result and number of comparisons.\n",
        "def Processing_input(posting_List1, posting_List2,operator):\n",
        "    if(operator == 'or'):\n",
        "        result, comparison = OR_operation(posting_List1, posting_List2) \n",
        "    if(operator == 'and'): \n",
        "        result, comparison = AND_operation(posting_List1, posting_List2) \n",
        "    if(operator == 'ornot'):\n",
        "        result, comparison = OR_NOT_operation(posting_List1, posting_List2)\n",
        "    if(operator == 'andnot'):\n",
        "        result, comparison= AND_NOT_operation(posting_List1, posting_List2) \n",
        "    return result, comparison"
      ],
      "metadata": {
        "id": "wrkMdESwnsV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function takes the number of queries and perform required operations.\n",
        "try:\n",
        "  comparisons=0\n",
        "  N=int(input('Enter the number of queries'))\n",
        "  for i in range(N):\n",
        "    query=EnterQuery()\n",
        "\n",
        "    operations=EnterOperations(len(query))\n",
        "    for i in range(len(operations)):\n",
        "        if(i == 0):\n",
        "            result = posting_list[query[0]]  \n",
        "            # num_comp=num_comp+num_comp_temp  \n",
        "        result,comp = Processing_input(result, posting_list[query[i+1]],operations[i])\n",
        "        comparisons=comparisons+comp\n",
        "    print(\"Number of documents retrieved:\",len(result))\n",
        "    print(\"Minimum number of comparisons done:\",comparisons)\n",
        "    document_names=[]\n",
        "    for i in result:\n",
        "      document_names.append(file_content.iloc[i,0])\n",
        "    print(\"The list of document names retrieved: \",document_names)\n",
        "except:\n",
        "  print('Enter Valid Input!!!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P84s2Jadkqxy",
        "outputId": "bfb127f6-1533-4e30-fafb-dc267accc46f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of queries2\n",
            "Enter your querylion stood thoughtfully for a moment\n",
            "Enter the operators list of 3 operators[ OR, OR , OR ]\n",
            "Number of documents retrieved: 56\n",
            "Minimum number of comparisons done: 183\n",
            "The list of document names retrieved:  ['conan.txt', 'coyote.txt', 'incarhel.hum', 'cartoon_.txt', 'ivan.hum', 'lbinter.hum', 'murphys.txt', 'llong.hum', 'lif&love.hum', 'lifeimag.hum', 'lozerzon.hum', 'luggage.hum', 'mailfrag.hum', 'maecenas.hum', 'm0dzmen.hum', 'mash.hum', 'meinkamp.hum', 'reasons.txt', 'montpyth.hum', 'news.hum', 'myheart.hum', 'cartoon.law', 'onetoone.hum', 'oldeng.hum', 'deep.txt', 'onetotwo.hum', 'pizzawho.hum', 'phorse.hum', 'peatchp.hum', 'passage.hum', 'policpig.hum', 'popmusi.hum', 'devils.jok', 'quest.hum', 'pro-fact.hum', 'radiolaf.hum', 'fuckyou2.txt', 'drunk.txt', 'smurfkil.hum', 'shuttleb.hum', 'socecon.hum', 'soleleer.hum', 'stone.hum', 'top10.txt', \"terrmcd'.hum\", 'timetr.hum', 'tfepisod.hum', 'lawyer.jok', 'hecomes.jok', 'let.go', 'cartoon.laws', 'wedding.hum', 'whoops.hum', 'wagon.hum', 'worldend.hum', 'strine.txt']\n",
            "Enter your querytelephone,paved, roads\n",
            "Enter the operators list of 2 operators[ OR NOT, AND NOT ]\n",
            "Number of documents retrieved: 983\n",
            "Minimum number of comparisons done: 2415\n",
            "The list of document names retrieved:  ['rinaldos.txt', 'bbq.txt', 'mothers.txt', 'harmful.hum', 'mowers.txt', 'hate.hum', 'herb!.hum', 'hell.jok', 'icm.hum', 'howlong.hum', 'hi.tec', 'conan.txt', 'murph.jok', 'roach.asc', 'imprrisk.hum', 'coyote.txt', 'horoscop.jok', 'incarhel.hum', 'insanity.hum', 'impurmat.hum', 'bible.txt', 'murphy.txt', 'insure.hum', 'cartoon_.txt', 'bless.bc', 'interv.hum', 'investi.hum', 'ivan.hum', 'japrap.hum', 'terms.hum', 'jac&tuu.hum', 'test.jok', 'dym', 'killer.hum', 'test2.jok', 'lawsuniv.hum', 'blooprs1.asc', 'kilsmur.hum', 'testchri.txt', 'kid_diet.txt', 'motrbike.jok', 'legal.hum', 'ludeinfo.txt', 'simp.txt', 'murphys.txt', 'livnware.hum', 'lll.hum', 'llong.hum', 'lobquad.hum', 'office.txt', 'lif&love.hum', 'lifeinfo.hum', 'lifeimag.hum', 'looser.hum', 'phunatdi.ana', 'catstory.txt', 'losers86.hum', 'ludeinfo.hum', 'luggage.hum', 'losers84.hum', 'lozeuser.hum', 'mailfrag.hum', 'manspace.hum', 'makebeer.hum', 'luzerzo2.hum', 'maecenas.hum', 'm0dzmen.hum', 'madscrib.hum', 'manilla.hum', 'marines.hum', 'mash.hum', 'meinkamp.hum', 'mtm.hum', 'pun.txt', 'memo.hum', 'miranda.hum', 'reasons.txt', 'miami.hum', 'miamadvi.hum', 'f_tang.txt', 'calvin.txt', 'missheav.hum', 'mutate.hum', 'misery.hum', 'mydaywss.hum', 'mrscienc.hum', 'naivewiz.hum', 'boston.geog', 'news.hum', 'newmex.hum', 'myheart.hum', 'newconst.hum', 'cartwb.son', 'nysucks.hum', 'nuke.hum', 'nurds.hum', 'cartoon.law', 'novel.hum', 'chunnel.txt', 'o-ttalk.hum', 'odearakk.hum', 'ohandre.hum', 'oldeng.hum', 'p-law.hum', 'deep.txt', 'opinion.hum', 'ookpik.hum', 'planeget.hum', 'pizzawho.hum', 'phony.hum', 'phorse.hum', 'parabl.hum', 'peatchp.hum', 'passage.hum', 'prawblim.hum', 'policpig.hum', 'popmusi.hum', 'devils.jok', 'poll2res.hum', 'popconc.hum', 'dieter.txt', 'prayer.hum', 'dead-r', 'quest.hum', 'psilaine.hum', 'raven.hum', 'radiolaf.hum', 'rapmastr.hum', 'ratspit.hum', 'reagan.hum', 'fuckyou2.txt', 'donut.txt', 'ratings.hum', 'drinkrul.jok', 'reddye.hum', 'research.hum', 'report.hum', 'docspeak.txt', 'repair.hum', 'rentals.hum', 'rocking.hum', 'ripoffpc.hum', 't_zone.jok', 'drunk.txt', 'corporat.txt', 'insult', 'skippy.hum', 'takenote.jok', 'cucumber.txt', 'shuttleb.hum', 'iqtest', 'socecon.hum', 'soleleer.hum', 'terbear.txt', 'teens.txt', 'spydust.hum', 'spider.hum', 'solviets.hum', 'social.hum', 'stone.hum', 'top10.txt', 'termpoem.txt', 'sungenu.hum', 'standard.hum', 'talebeat.hum', 'languag.jok', 'televisi.hum', 'taping.hum', 'y.txt', 'texican.dic', \"terrmcd'.hum\", 't-shirt.hum', 't-10.hum', 'test.hum', 'timetr.hum', 'tickmoon.hum', 'terrnieg.hum', 'textgrap.hum', 'thecube.hum', 'hecomes.jok', 'let.go', 'koans.txt', 'toxcwast.hum', 'turbo.hum', 'truthlsd.hum', 'thesis.beh', 'truths.hum', 'tribble.hum', 'thermite.ana', 'voltron.hum', 'trekwes.hum', 'cartoon.laws', 'twinkie.txt', 'whoon1st.hum', 'why-me.hum', 'wedding.hum', 'wetdream.hum', 'whoops.hum', 'wagon.hum', 'zodiac.hum', 'worldend.hum', 'yjohncse.hum', 'billcat.hum', 'xtermin8.hum', 'who.txt', 'yuppies.hum', 'word.hum', 'limerick.jok', 'trukdeth.txt', 'hotnnot.hum', 'jeffie.heh', 'liceprof.sty', 'lines.jok', 'tshirts.jok', 'units.mea', 'st_silic.txt', 'univ.odd', 'waitress.txt', 'wagit.txt', 'washroom.txt', 'vaguemag.90s', 'weight.txt', 'lion.jok', 'welfare.txt', 'marriage.hum', 'math.2', 'yogisays.txt', 'wimptest.txt', 'math.1', 'woodbugs.txt', 'math.far', 'lozers', 'zgtoilet.txt', 'chainltr.txt', 'woolly_m.amm', 'climbing.let', 'childhoo.jok', 'co-car.jok', 'church.sto', 'netnews.10', 'nigel.2', 'nigel.10', 'court.quips', 'element.jok', 'eandb.drx', 'electric.txt', 'elephant.fun', 'hack', 'fascist.txt', 'idr2.txt', 'fartting.txt', 'exam.50', 'drinker.txt', 'fartinfo.txt', 'jokes.txt', 'final-ex.txt', 'ourfathr.txt', 'nigel.4', 'fuck!.txt', 'nigel.7', 'fusion.sup', 'lawhunt.txt', 'lawskool.txt', 'grammar.jok', 'gas.txt', 'rec.por', 'gd_guide.txt', 'german.aut', 'grospoem.txt', 'good.txt', 'passenge.sim', 'potty.txt', 'polly.txt', 'nintendo.jok', 'pepsideg.txt', 'poli_t.ics', 'pickup.lin', 'polemom.txt', 'moonshin', 'problem.txt', 'dover.poem', 'pournell.spo', 'polly_.new', 'proof.met', 'psalm_re.aga', 'proposal.jok', 'psych_pr.quo', 'prover_w.iso', 'pukeprom.jok', 'psalm23.txt', 'prooftec.txt', 'pure.mat', 'peanuts.txt', 'curse.txt', 'college.hum', 'tuflife.txt', 'leech.txt', 'puzzles.jok', 'python_s.ong', 'smartass.txt', 'pickup.txt', 'pecker.txt', 'eskimo.nel', 'flowchrt.txt', 'height.txt', 'pipespec.txt', 'hilbilly.wri', 'hitler.59', 'hitlerap.txt', 'how_to_i.pro', 'smokers.txt', 'snipe.txt', 'stereo.txt', 'horoscop.txt', 'htswfren.txt', 'steroid.txt', 'sysadmin.txt', 'inlaws1.txt', 'law.sch', 'televisi.txt', 'humatra.txt', 'humatran.jok', 'insuranc.sty', 'idaho.txt', 'psycho.txt', 'italoink.txt', 'jokeju07.txt', 'woodbine.txt', 'kloo.txt', 'jargon.phd', 'realest.txt', 'jc-elvis.inf', 'laws.txt', 'lampoon.jok', 'letgosh.txt', 'letter_f.sch', 'sysman.txt', 'odd_to.obs', 'farsi.phrase', 'lions.cat', 'luvstory.txt', 'lucky.cha', 'nukwaste', 'socks.drx', 'signatur.jok', 'one.par', 'smiley.txt', 'primes.jok', 'oxymoron.jok', 'smurfs.cc', 'spelin_r.ifo', 'princess.brd', 'studentb.txt', 'startrek.txt', 'humpty.dumpty', 'jrrt.riddle', 'telecom.q', 'mr.rogers', 'puzzle.spo', 'poli.tics', 'un.happy', 'shrink.news', 'psalm.reagan', 'number.killer', 'prover.wisom', 'psalm_nixon', 'quantum.jok', 'rent-a_cat', 'popmach', 'resrch_phrase', 'quick.jok', 'shorties.jok', 'quotes.bug', 'quotes.jok', 'racist.net', 'rinaldos.law', 'top10.elf', 'squids.gph', 'texican.lex', 'tnd.1', 'welfare', 'temphell.jok', 'aussie.lng', 'bw-phwan.hat', 'wisdom', 'bw-summe.hat', 'justify', 'la_times.hun', 'speling.msk', 'outawork.erl', 'firstaid.inf', 'supermar.rul', 'ambrose.bie', 'oldtime.sng', 'oasis', 'oxymoron.txt', 'gaiahuma', 'kid2', 'anim_lif.txt', 'bad', 'talkbizr.txt', 'wood', 'woodsmok.txt', 'bagelope.txt', 'empeval.txt', 'variety2.asc', 'variety3.asc', 'epitaph', 'english', 'engrhyme.txt', 'various.txt', 'excuse30.txt', 'vegkill.txt', 'vonthomp', 'q.pun', 'godmonth.txt', 'growth.txt', 'gown.txt', 'cogdis.txt', 'mrsfield', 'b12.txt', 'beginn.ers', 'newcoke.txt', 'candybar.fun', 'hamburge.nam', 'snapple.rum', 'chili.txt', 'jerky.rcp', 'vegan.rcp', 'mead.rcp', 'coke1', 'bakebred.txt', 'coke.txt', 'cooking.fun', 'coladrik.txt', 'coladrik.fun', 'tuna.lab', 'turkey.fun', 'fiber.txt', 'bread.txt', 'appetiz.rcp', 'recepies.fun', 'fajitas.rcp', 'x-drinks.txt', 'oculis.rcp', 'oatbran.rec', 'pepper.txt', 'coke_fan.naz', 'fudge.txt', 'coke.fun', 'kashrut.txt', 'curry.hrb', 'choco-ch.ips', 'hotel.txt', 'dthought.txt', 'tpquote2.txt', 'tpquotes.txt', 'gingbeer.txt', 'gameshow.txt', 'cooking.jok', 'rinaldo.jok', 'women.jok', 'wrdnws1.txt', 'wrdnws2.txt', 'wrdnws4.txt', 'wrdnws5.txt', 'wrdnws8.txt', 'wrdnws9.txt', 'letterbx.txt', 'hierarch.txt', 'ads.txt', 'epikarat.txt', 'namm', 'kanalx.txt', 'adt_miam.txt', 'gd_flybd.txt', 'earp', 'widows', 'twinpeak.txt', 'lazarus.txt', 'barney.cn1', 'bb', 'bimg.prn', 'bmdn01.txt', 'libraway.txt', 'twinkies.jok', 'sanshop.txt', 'seeds42.txt', 'top10st1.txt', 'top10st2.txt', 'transp.txt', 'johann', 'just2', 'bnb_quot.txt', 'bored.txt', 'sw_err.txt', 'skippy.txt', 'recip1.txt', 'shooters.txt', 'slogans.txt', 'smurf-03.txt', 'smurf_co.txt', 'soccer.txt', 'beerjesus.hum', 'bozo_tv.leg', 'coffee.txt', 'diet.txt', 'econridl.fun', 'oliver.txt', 'oliver02.txt', 'sf-zine.pub', 'topten.hum', 'how2bgod.txt', 'hangover.txt', 'netmask.txt', 'chickens.txt', 'classicm.hum', 'cmu.share', 'cookbkly.how', 'fearcola.hum', 'hedgehog.txt', 'horflick.txt', 'alcohol.hum', 'beer-g', 'beergame.hum', 'iced.tea', 'thievco.txt', 'aeonint.txt', 'deathhem.txt', 'bread.rec', 'brownie.rec', 'lion.txt', 'lp-assoc.txt', 'desk.txt', 'flowchrt', 'packard.txt', 'parsnip.txt', 'penisprt.txt', 'msfields.txt', 'mtv.asc', 'modstup', 'moslem.txt', 'mov_rail.txt', 'miamimth.txt', 'food', 'foodtips', 'cake.rec', 'docdict.txt', 'drinks.txt', 'bhang.fun', 'booze1.fun', 'booze2.fun', 'cgs_lst.txt', 'golnar.txt', 'red-neck.jks', 'reddwarf.sng', 'renorthr.txt', 'rns_bcl.txt', 'rns_bwl.txt', 'rns_ency.txt', 'robot.tes', 'staff.txt', 'fusion.gal', 'sfmovie.txt', 'the_ant.txt', 'mog-history', 'dalive', 'iremember', 'kilroy', 'cform2.txt', 'msorrow', 'nameisreo.txt', 'inquirer.txt', 'ins1', 'buldrwho.txt', 'episimp2.txt', 'epi_merm.txt', 'epi_rns.txt', 'epi_tton.txt', 'gd_drwho.txt', 'twilight.txt', 'gd_gal.txt', 'outlimit.txt', 'lost.txt', 'gd_liqtv.txt', 'gd_hhead.txt', 'epiquest.txt', 'gd_frasr.txt', 'gd_maxhd.txt', 'gd_ol.txt', 'ghostsch.hum', 'gd_sgrnd.txt', 'a_fish_c.apo', 'a_tv_t-p.com', 'adrian_e.faq', 'allusion', 'anime.lif', 'hbo_spec.rev', 'hitchcok.txt', 'wacky.ani', 'wkrp.epi', 'cast.lis', 'christop.int', 'chung.iv', 'clancy.txt', 'maxheadr', 'facedeth.txt', 'farsi.txt', 'nukewar.txt', 'number', 'spoonlis.txt', 'sinksub.txt', 'sorority.gir', 'petshop', 'pracjoke.txt', 'fed.txt', 'cybrtrsh.txt', 'homermmm.txt', 'how2dotv.txt', 'fish.rec', 'hitchcoc.app', 'fegg!int.txt', 'feggmagi.txt', 'normquot.txt', 'history2.oop', 'pol-corr.txt', 'poopie.txt', 'ppbeer.txt', 'pot.txt', 'headlnrs', 'hell.txt', 'hum2', 'cokeform.txt', 'contract.moo', 'cookberk', 'cooplaws', 'cuisine.txt', 'coffee.faq', 'minn.txt', 'oam-001.txt', 'oam.nfo', 'acronym.txt', 'college.txt', 'english.txt', 'figure_1.txt', 'crzycred.lst', 'arnold.txt', 'avengers.lis', 'bbc_vide.cat', 'blake7.lis', 'doc-says.txt', 'dromes.txt', 'eatme.txt', 'free-cof.fee', 'initials.rid', 'old.txt', 'post.nuc', 'quantum.phy', 'resolutn.txt', 'soporifi.abs', 'swearfrn.hum', 'c0dez.txt', 'beer.txt', 'butcher.txt', 'curiousgeorgie.txt', 'freudonseuss.txt', 'normalboy.txt', 'annoy.fascist', 'booze.fun', 'diesmurf.txt', 'elevator.fun', 'get.drunk.cheap', 'how.bugs.breakd', 'normal.boy', 'dead4.txt', 'radexposed.txt', 'pat.txt', 'bugs.txt', 'hack7.txt', 'happyhack.txt', 'airlines', 'skincat', 'venganza.txt', 'venison.txt', 'hop.faq', 'hotpeper.txt', 'necropls.txt', 'three.txt', 'egglentl.vgn', 'dandwine.bev', 'eggroll1.mea', 'egg-bred.txt', 'engmuffn.txt', 'frogeye1.sal', 'focaccia.brd', 'firecamp.txt', 'feista01.dip', 'garlpast.vgn', 'goldwatr.txt', 'greenchi.txt', 'texbeef.txt', 'whitbred.txt', 'wonton.txt', 'gack!.txt', 'unochili.txt', 'woods.txt', 'yogurt.asc', 'damiana.hrb', 'calamus.hrb', 'imbecile.txt', 'pbcookie.des', 'pasta001.sal', 'oranchic.pol', 'penndtch', 'orgfrost.bev', 'jon.txt', 'whatbbs', 'hermsys.txt', 'awespinh.sal', 'apsaucke.des', 'appbred.brd', 'arcadian.txt', 'antimead.bev', 'applepie.des', 'banana03.brd', 'banana01.brd', 'banana02.brd', 'batrbred.txt', 'beershrm.fis', 'banana05.brd', 'beershrp.fis', 'banana04.brd', 'baklava.des', 'jawgumbo.fis', 'berryeto.bev', 'jalapast.dip', 'jawsalad.fis', 'jambalay.pol', 'japice.bev', 'jungjuic.bev', 'strattma.txt', 'zucantom.sal', 'montoys.txt', 'seafood.txt', 'stagline.txt', 'mitch.txt', 'zuccmush.sal', 'margos.txt', 'shuimai.txt', 'blkbean.txt', 'blkbnsrc.vgn', 'btscke03.des', 'boarchil.txt', 'bredcake.des', 'btscke01.des', 'breadpud.des', 'brdpudd.des', 'btscke02.des', 'btscke05.des', 'btscke04.des', 'burrito.mea', 'caesardr.sal', 'bunacald.fis', 'butstcod.fis', 'capital.txt', 'buffwing.pol', 'caramels.des', 'advrtize.txt', 'nzdrinks.txt', 'recipe.003', 'recipe.002', 'oldtime.txt', 'recipe.007', 'recipe.004', 'recipe.005', 'recipe.010', 'oakwood.txt', 'recipe.008', 'recipe.011', 'recipe.006', 'recipe.009', 'renored.txt', 'recipe.001', 'richbred.txt', 'recipe.012', 'all_grai', 'amchap2.txt', 'beer.gam', 'beergame.txt', 'beer-gui', 'gotukola.hrb', 'hitler.txt', 'coollngo2.txt', 'jimhood.txt', 'aniherb.txt', 'antibiot.txt', 'brush1.txt', 'booknuti.txt', 'blood.txt', 'ayurved.txt', 'chinese.txt', 'cereal.txt', 'critic.txt', 'arthriti.txt', 'bw.txt', 'curry.txt', 'aphrodis.txt', 'anorexia.txt', 'acetab1.txt', 'atherosc.txt', 'acne1.txt', 'beesherb.txt', 'back1.txt', '1st_aid.txt', 'proudlyserve.txt', 'jayjay.txt', 'att.txt', 'calculus.txt', 'btaco.txt', 'byfb.txt', 'beerwarn.txt', 'gumbo.txt', 'qttofu.vgn', 'paddingurpapers.txt', 'aggie.txt', 'coffeebeerwomen.txt', 'childrenbooks.txt', 'cartoon_laws.txt', 'llamas.txt', 'religion.txt', 'horoscope.txt', 'confucius_say.txt', 'ganamembers.txt', 'fireplacein.txt', 'enlightenment.txt', 'girlspeak.txt', 'labels.txt', 'trekfume.txt', 'turing.shr', 'valujet.txt', 'computer.txt', 'jokes1.txt', 'lawyers.txt', 'cops.txt', 'fbipizza.txt', 'mcd.txt', 'nasaglenn.txt', 'bhb.ill', 'bond-2.txt', 'indgrdn.txt', 'insect1.txt', 'quantity.001', 'sawyer.txt', 'exidy.txt', 'deadlysins.txt', 'yuban.txt', 'planetzero.txt', 'moore.txt', 'chickenheadbbs.txt', 'phxbbs-m.txt', 'silverclaws.txt', 'namaste.txt', 'crazy.txt', 'lifeonledge.txt', 'lipkovits.txt', 'basehead.txt', 'draxamus.txt', 'stressman.txt', 'exylic.txt', 'apsnet.txt', 'lansing.txt', 'heroic.txt', 'onan.txt', 'teevee.hum', 'hacktest.txt', 'grail.txt', 'mel.txt', 'hackingcracking.txt', 'adameve.hum', 'analogy.hum', 'bingbong.hum', 'charity.hum', 'drugshum.hum', 'modest.hum', 'cheapfar.hum', 'reconcil.hum', 'parades.hum', 'ghostfun.hum', 'saveface.hum', 'poets.hum', 'shameonu.hum', 'whatthe.hum', 'symbol.hum', 'solders.hum', 'brainect.hum', 'weights.hum', 'watchlip.hum', 'spacever.hum', 'kaboom.hum', 'memory.hum', 'adcopy.hum', 'zen.txt', 'b-2.jok', 'abbott.txt', 'manager.txt', 'admin.txt', 'suicide2.txt', 'alcatax.txt', 'nigel10.txt', 'atombomb.hum', 'alflog.txt', 'badday.hum', 'boatmemo.jok', 'bank.rob', 'alabama.txt', 'cancer.rat', 'answers', 'catin.hat', 'beave.hum', 'beapimp.hum', 'beer.hum', 'bitchcar.hum', 'argotdic.txt', 'blackapp.hum', 'bigpic1.hum', 'blackhol.hum', 'cold.fus', 'blaster.hum', 'browneco.hum', 'cheapin.la', 'boe.hum', 'bugbreak.hum', 'btcisfre.hum', 'butwrong.hum', 'chickens.jok', 'cabbage.txt', 'buzzword.hum', 'merry.txt', 'cockney.alp', 'calif.hum', 'catballs.hum', 'change.hum', 'cbmatic.hum', 'macsfarm.old', 'chinesec.hum', 'catranch.hum', 'making_y.wel', 'nukewar.jok', 'college.sla', 'coldfake.hum', 'madhattr.jok', 'comrevi1.hum', 'malechem.txt', 'cucumber.jok', 'number_k.ill', 'arab.dic', 'addrmeri.txt', 'cowexplo.hum', 'rabbit.txt', 'anthropo.stu', 'failure.txt', 'd-ned.hum', 'dark.suc', 'dingding.hum', 'deterior.hum', 'druggame.hum', 'defectiv.hum', 'art-fart.hum', 'disaster.hum', 'engineer.hum', 'bad-d', 'age.txt', 'murphy_l.txt', 'doggun.sto', 'bad.jok', 'beerdiag.txt', 'record_.gap', 'firstaid.txt', 'drinking.tro', 'relative.ada', 'footfun.hum', 'finalexm.hum', 'flattax.hum']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GlIPrhsPvLD-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}